# -*- coding: utf-8 -*-
"""Financial_Analysis_&_Automation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/team-headstart/Financial-Analysis-and-Automation-with-LLMs/blob/main/Financial_Analysis_%26_Automation.ipynb

![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)

# Headstarter Financial Analysis & Automation with LLMs Project

# Install Libraries
"""

# ! pip install yfinance langchain_pinecone openai python-dotenv langchain-community sentence_transformers

from dotenv import load_dotenv
from groq import Groq
from langchain_pinecone import PineconeVectorStore
import numpy as np
from openai import OpenAI
# import dotenv
import json
import yfinance as yf
import concurrent.futures
from langchain_community.embeddings import HuggingFaceEmbeddings
# from google.colab import userdata
from langchain.schema import Document
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
import requests
import os
#customer import
from modules.get_stock_info import get_stock_info
from modules.get_company_tickets import get_company_tickers
# from modules.process_stock import parallel_process_stocks
from NLP_utils.get_cos_similarity import cosine_similarity_between_sentences
from NLP_utils.get_huggingface_embeddings import get_huggingface_embeddings
from threading import Lock
load_dotenv()
pinecone_api_key = os.getenv("PINECONE_API_KEY")
groq_api_key = os.getenv("GROQ_API_KEY")


data = yf.Ticker("NVDA")
stock_info = data.info
# print(stock_info)



# Example usage
sentence1 = "I like walking to the park"
sentence2 = "I like running to the office"

similarity = cosine_similarity_between_sentences(sentence1, sentence2)

aapl_info = get_stock_info('AAPL')
# print(aapl_info)

aapl_description = aapl_info['Business Summary']

company_description = "I want to find companies that make smartphones and are headquarted in California"

similarity = cosine_similarity_between_sentences(aapl_description, company_description)





# """# Get all the Stocks in the Stock Market

# First, we need to get the symbols (also known as tickers) of all the stocks in the stock market
# """



company_tickers = get_company_tickers()

# company_tickers

# len(company_tickers)



# """# Inserting Stocks into Pinecone

# **1. Create an account on [Pinecone.io](https://app.pinecone.io/)**

# **2. Create a new index called "stocks" and set the dimensions to 768. Leave the rest of the settings as they are.**

# ![Screenshot 2024-12-02 at 4 48 03â€¯PM](https://github.com/user-attachments/assets/13b484da-cd00-4337-a4db-4779080eb42c)


# **3. Create an API Key for Pinecone**

# ![Screenshot 2024-11-24 at 10 44 37 PM](https://github.com/user-attachments/assets/e7feacc6-2bd1-472a-82e5-659f65624a88)


# **4. Store your Pinecone API Key within Google Colab's secrets section, and then enable access to it (see the blue checkmark)**


# ![Screenshot 2024-11-24 at 10 45 25 PM](https://github.com/user-attachments/assets/eaf73083-0b5f-4d17-9e0c-eab84f91b0bc)



# """

# pinecone_api_key = userdata.get("PINECONE_API_KEY")
# os.environ['PINECONE_API_KEY'] = pinecone_api_key

index_name = "stocks"
namespace = "stock-descriptions"

hf_embeddings = HuggingFaceEmbeddings()
vectorstore = PineconeVectorStore(index_name=index_name, embedding=hf_embeddings)

# """# Sequential Processing

# It will take very long to process all the stocks like this!

# [![](https://mermaid.ink/img/pako:eNqNkl1rgzAUhv9KyMXYwF74cSVjYA0rhY6WKRQWe5FqWqU1cUm8GKX_ffmwa65Gc6Hn5LxvzmM8F1jzhsIUHgUZWlCiigG9MlwoXp_AqpPqdS_esmyzCsBivV7o10fxXgagLFbZDsxmb2COi44dzxRsBK-plDoBZSsoaXbuNPeU4941qWBBv0fKVEfOvud5yejh0NWdLr1U0LnMmts2eYhDMLsZgEHa3TV5aEXbEG-zZZnasiXfLMGnaScVeNKRHDiT1DNunRGF5pMFtUaAiCKe5h4hp84jHHks9mJ8mMjBRBOMrT9G45wommis84bjYThZHuPYwzA_xqeIHUU8UZjyYxDOiOIJwhj_uRKnzhOc3FHsdHgoiUNJJhRTfgzFGVEyoRijj0JZAwPYU9GTrtFjfDHbFVQt7WkFUx02RJzMMF21joyKFz-shqkSIw2g4OOxhemBnKXOxqEhiqKO6DHt_3YHwr44v-XXX7It6B4?type=png)](https://mermaid.live/edit#pako:eNqNkl1rgzAUhv9KyMXYwF74cSVjYA0rhY6WKRQWe5FqWqU1cUm8GKX_ffmwa65Gc6Hn5LxvzmM8F1jzhsIUHgUZWlCiigG9MlwoXp_AqpPqdS_esmyzCsBivV7o10fxXgagLFbZDsxmb2COi44dzxRsBK-plDoBZSsoaXbuNPeU4941qWBBv0fKVEfOvud5yejh0NWdLr1U0LnMmts2eYhDMLsZgEHa3TV5aEXbEG-zZZnasiXfLMGnaScVeNKRHDiT1DNunRGF5pMFtUaAiCKe5h4hp84jHHks9mJ8mMjBRBOMrT9G45wommis84bjYThZHuPYwzA_xqeIHUU8UZjyYxDOiOIJwhj_uRKnzhOc3FHsdHgoiUNJJhRTfgzFGVEyoRijj0JZAwPYU9GTrtFjfDHbFVQt7WkFUx02RJzMMF21joyKFz-shqkSIw2g4OOxhemBnKXOxqEhiqKO6DHt_3YHwr44v-XXX7It6B4)
# """

# for idx, stock in company_tickers.items():
#     stock_ticker = stock['ticker']
#     stock_data = get_stock_info(stock_ticker)
#     stock_description = stock_data['Business Summary']

#     print(f"Processing stock {idx} / {len(company_tickers)} :", stock_ticker)

#     vectorstore_from_documents = PineconeVectorStore.from_documents(
#         documents=[Document(page_content=stock_description, metadata=stock_data)],
#         embedding=hf_embeddings,
#         index_name=index_name,
#         namespace=namespace
#     )

# """# Parallelizing

# [![](https://mermaid.ink/img/pako:eNqFk0uLgzAQgP_KkMOe7MHXRZaC1baXvsDCwqqHrGarVJMSE9hS-983NnWxULceRmf4Pp2MyQVlLCfIQweOTwXsw4SCuvw4Eiw7wqpsxPsXn_r-bmXAcrtdqts6WuwN2Ecr3wB__blJYTKZwixe45JCwKjgrKoI7zwXdphjlVXwwfiR8CbVH9BxdjPbqKxlJTAlTDbVuYXAjDUNZveSHWcZaRromkj_F61etIbire8Xpt2b9tDslvpCdHrRGYrddF6Ibi-6D4vsBjqcUWDe_NCMl0TAG6gfwwmEWOA7FlgasEYBWwP2KOBowBkFXA24Y4COoW61DRklLcwvQUHUHooEFrK53hHrAbkX7WdF51nRfVLUcX4fs8y6ObawMON-phvyI2CGRVakyEA14TUuc7XnL52ZIFGQmiTIU4855scEJfSqOCwFi840Q57gkhiIM3kokPeNq0Zl8pRjQcISq4NT_1VPmH4y1ufXXwdkCM8?type=png)](https://mermaid.live/edit#pako:eNqFk0uLgzAQgP_KkMOe7MHXRZaC1baXvsDCwqqHrGarVJMSE9hS-983NnWxULceRmf4Pp2MyQVlLCfIQweOTwXsw4SCuvw4Eiw7wqpsxPsXn_r-bmXAcrtdqts6WuwN2Ecr3wB__blJYTKZwixe45JCwKjgrKoI7zwXdphjlVXwwfiR8CbVH9BxdjPbqKxlJTAlTDbVuYXAjDUNZveSHWcZaRromkj_F61etIbire8Xpt2b9tDslvpCdHrRGYrddF6Ibi-6D4vsBjqcUWDe_NCMl0TAG6gfwwmEWOA7FlgasEYBWwP2KOBowBkFXA24Y4COoW61DRklLcwvQUHUHooEFrK53hHrAbkX7WdF51nRfVLUcX4fs8y6ObawMON-phvyI2CGRVakyEA14TUuc7XnL52ZIFGQmiTIU4855scEJfSqOCwFi840Q57gkhiIM3kokPeNq0Zl8pRjQcISq4NT_1VPmH4y1ufXXwdkCM8)
# """

# Initialize tracking lists
successful_tickers = []
unsuccessful_tickers = []

# Load existing successful/unsuccessful tickers
try:
    with open('successful_tickers.txt', 'r') as f:
        successful_tickers = [line.strip() for line in f if line.strip()]
    # print(f"Loaded {len(successful_tickers)} successful tickers")
except FileNotFoundError:
    print("No existing successful tickers file found")

try:
    with open('unsuccessful_tickers.txt', 'r') as f:
        unsuccessful_tickers = [line.strip() for line in f if line.strip()]
    # print(f"Loaded {len(unsuccessful_tickers)} unsuccessful tickers")
except FileNotFoundError:
    print("No existing unsuccessful tickers file found")



# Initialize Pinecone
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"],environment="us-west1-gcp")

# Connect to your Pinecone index
pinecone_index = pc.Index(index_name)

# def rank_stocks_by_query(query, top_k=5):
#     """
#     Finds the most similar stocks to a user query using cosine similarity and Hugging Face embeddings.

#     Args:
#         query (str): User's natural language query.
#         vectorstore: Pinecone vector store instance.
#         top_k (int): Number of top similar stocks to return.

#     Returns:
#         list: Ranked list of the top-k most similar stocks with their similarity scores and metadata.
#     """
#     # Generate embedding for the query and convert to a Python list
#     query_embedding = get_huggingface_embeddings(query).tolist()

#     ranked_stocks = []

#     # Iterate through successfully processed tickers
#     idx=0
#     for ticker in successful_tickers:
#         print(f"tickers batch :{idx}")
#         idx+=1
#         try:
#             # Retrieve the stock embedding and metadata from Pinecone
#             result = pinecone_index.query(
#                 vector=query_embedding,  # Ensure the embedding is a Python list
#                 top_k=1,  # We want data for the specific ticker
#                 include_metadata=True,
#                 namespace=namespace
#             )

#             if result['matches']:
#                 match = result['matches'][0]
#                 metadata = match['metadata']
#                 score = match['score']  # Pinecone already provides a similarity score

#                 # Append the result to the ranked stocks
#                 ranked_stocks.append({
#                     "Ticker": ticker,
#                     "Score": score,
#                     "Details": metadata
#                 })

#         except Exception as e:
#             print(f"Error retrieving data for {ticker}: {e}")

#     # Sort by similarity score in descending order
#     ranked_stocks = sorted(ranked_stocks, key=lambda x: x['Score'], reverse=True)

#     return ranked_stocks[:top_k]
def rank_stocks_by_query_parallel(query, top_k=5, max_workers=15):
    """
    Finds the most similar stocks to a user query using cosine similarity and Hugging Face embeddings.
    Implements parallelism to query multiple tickers simultaneously.

    Args:
        query (str): User's natural language query.
        top_k (int): Number of top similar stocks to return.
        max_workers (int): Number of threads for parallel processing.

    Returns:
        list: Ranked list of the top-k most similar stocks with their similarity scores and metadata.
    """
    # Generate embedding for the query and convert to a Python list
    query_embedding = get_huggingface_embeddings(query).tolist()

    ranked_stocks = []
    lock = Lock()
    ticker_index = 0
    def process_ticker(ticker):
        """
        Helper function to process a single ticker and query Pinecone.
        """
        nonlocal ticker_index
        try:
            # Query Pinecone for the ticker
            result = pinecone_index.query(
                vector=query_embedding,  # Ensure the embedding is a Python list
                top_k=1,  # Fetch data for the specific ticker
                include_metadata=True,
                namespace=namespace
            )

            if result['matches']:
                match = result['matches'][0]
                metadata = match.get('metadata', {})
                score = match['score']
                with lock:  # Safely update and print the index
                    print(f"Processing index: {ticker_index}, Ticker: {ticker}")
                    ticker_index += 1
                return {
                    "Ticker": ticker,
                    "Score": score,
                    "Details": metadata
                }

        except Exception as e:
            print(f"Error retrieving data for {ticker}: {e}")
            return None

    # Use ThreadPoolExecutor for parallel processing
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_ticker = {
            executor.submit(process_ticker, ticker): ticker for ticker in successful_tickers
        }

        for future in concurrent.futures.as_completed(future_to_ticker):
            result = future.result()
            if result:
                ranked_stocks.append(result)

    # Sort results by similarity score in descending order
    ranked_stocks = sorted(ranked_stocks, key=lambda x: x['Score'], reverse=True)

    return ranked_stocks[:top_k]


# Example usage
# query = "What are some companies that manufacture consumer hardware?"
# ranked_results = rank_stocks_by_query_parallel(query, top_k=5)

# Print results
# print("Ranked Stocks:")
# for result in ranked_results:
#     print(f"{result['Ticker']}: {result['Score']:.4f}")
#     print(f"Details: {result['Details']}")

# # Prepare your tickers
# tickers_to_process = [company_tickers[num]['ticker'] for num in company_tickers.keys()]

# # Process them
# # parallel_process_stocks(tickers_to_process, max_workers=10)

# """# Perform RAG"""

# # Initialize Pinecone
# pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"],environment="us-west1-gcp")

# # Connect to your Pinecone index
# pinecone_index = pc.Index(index_name)

# query = "What are some companies that manufacture consumer hardware?"

# raw_query_embedding = get_huggingface_embeddings(query)

# top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)

# top_matches

# contexts = [item['metadata']['text'] for item in top_matches['matches']]

# augmented_query = "<CONTEXT>\n" + "\n\n-------\n\n".join(contexts[ : 10]) + "\n-------\n</CONTEXT>\n\n\n\nMY QUESTION:\n" + query

# print(augmented_query)

# """# Setting up Groq for RAG

# 1. Get your Groq API Key [here](https://console.groq.com/keys). Through Groq, you get free access to various LLMs

# 2. Paste your Groq API Key into your Google Colab secrets, and make sure to enable permissions for it

# ![Screenshot 2024-11-25 at 12 00 16 AM](https://github.com/user-attachments/assets/e5525d29-bca6-4dbd-892b-cc770a6b281d)
# """

# client = OpenAI(
#   base_url="https://api.groq.com/openai/v1",
#   api_key=os.environ["GROQ_API_KEY"]
# )

# system_prompt = f"""You are an expert at providing answers about stocks. Please answer my question provided.
# """

# llm_response = client.chat.completions.create(
#     model="llama-3.1-70b-versatile",
#     messages=[
#         {"role": "system", "content": system_prompt},
#         {"role": "user", "content": augmented_query}
#     ]
# )

# response = llm_response.choices[0].message.content

# print(response)
client = Groq(
    api_key=groq_api_key,
)
def generate_response_with_llm(query, top_k):
    ranked_stocks=rank_stocks_by_query_parallel(query, top_k=top_k)
     # Build context from ranked stocks
    context = "\n".join([f"{stock['Ticker']}: {stock['Details']['Business Summary']}" for stock in ranked_stocks])

    # Create augmented query
    augmented_query = f"<CONTEXT>\n{context}\n</CONTEXT>\n\nMY QUESTION:\n{query}"
    # print(f"augmented query is: \n{augmented_query}")
    # Query the LLM
    try:
        system_prompt = f"""You are an expert at providing answers about stocks. Please answer my question provided.
        """

        chat_creation = client.chat.completions.create(
            model="llama-3.1-70b-versatile",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": augmented_query}
            ]
        )
        response = chat_creation.choices[0].message.content
        return response
    except Exception as e:
        return f"Error generating response: {e}"
# response = generate_response_with_llm(query, ranked_results)
# print("Response with LLM")
# print(response)

import streamlit as st
st.title("Stock Research Automation with AI")
st.write("Query stocks using natural language and get AI-powered answers!")
query = st.text_input("Enter your query (e.g., 'What companies build data centers?')", "")
top_k = st.slider("Number of results to show", 1, 10, 5)
if st.button("Search"):
    if query:
        # tickers_list = [ticker.strip().upper() for ticker in tickers.split(",")]
        st.write("Processing your request...")
        response = generate_response_with_llm(query, top_k=top_k)
        st.write("### AI-Powered Response:")
        st.write(response)
    else:
        st.error("Please provide both a query and at least one stock ticker.")